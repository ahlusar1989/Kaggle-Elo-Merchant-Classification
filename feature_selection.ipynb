{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd6912b0e9c5d09def2f485329cc5332f0b2d593"
   },
   "source": [
    "# Feature Engineering + LightGBM Model with Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6dd24ff1fa7479bde7e4b8314e04f02cccd4fb4b"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "38e86350d9f5060ef5e2fd62673da0a4bd5eb9e0"
   },
   "source": [
    "Reference\n",
    "\n",
    "https://www.kaggle.com/fabiendaniel/elo-world\n",
    "\n",
    "https://www.kaggle.com/ashishpatel26/repeated-kfold-approach-rmse-3-70\n",
    "\n",
    "https://www.kaggle.com/chauhuynh/my-first-kernel-3-699/\n",
    "\n",
    "https://www.kaggle.com/yhn112/data-exploration-lightgbm-catboost-lb-3-760\n",
    "\n",
    "https://www.kaggle.com/nikitsoftweb/you-re-going-to-want-more-categories-lb-3-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b2dcee2d59f223b518a07a05a8f51fb70f6752f"
   },
   "outputs": [],
   "source": [
    "# loading original data \n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "histdata = pd.read_csv(\"../input/historical_transactions.csv\")\n",
    "newdata = pd.read_csv(\"../input/new_merchant_transactions.csv\")\n",
    "merchants = pd.read_csv(\"../input/merchants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d37c4dfc4acafcede474209206bd3907cf038f0"
   },
   "outputs": [],
   "source": [
    "# for submition\n",
    "lgb_submition = pd.DataFrame({\"card_id\":test[\"card_id\"].values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a6c2ecc0b00c88df554cc0706e720071c59b6eb"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35770d76122e6914bd7fde649489d3d6e1fa4b9f"
   },
   "outputs": [],
   "source": [
    "# train & test data\n",
    "# extract target value\n",
    "target = train['target']\n",
    "\n",
    "# Convert time as features\n",
    "for data in [train,test]:\n",
    "    data['first_active_month'] = pd.to_datetime(data['first_active_month'])\n",
    "    data['year'] = data['first_active_month'].dt.year\n",
    "    data['month'] = data['first_active_month'].dt.month\n",
    "    data['howlong'] = (datetime.date(2018,2,1) - data['first_active_month'].dt.date).dt.days\n",
    "\n",
    "train = train.drop(['first_active_month','target'], axis=1)\n",
    "test = test.drop(['first_active_month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d38da3f0af26aec3bd43b0bd037ba4bfbb8db8d"
   },
   "outputs": [],
   "source": [
    "# Convert category values\n",
    "def category_convert(data):\n",
    "    data['cat2'] = data['category_2']\n",
    "    data['cat3'] = data['category_3']\n",
    "    data = pd.get_dummies(data, columns=['cat2', 'cat3'])\n",
    "    for bi_cat in ['authorized_flag', 'category_1']:\n",
    "        data[bi_cat] = data[bi_cat].map({'Y':1, 'N':0})\n",
    "    return data\n",
    "\n",
    "histdata = category_convert(histdata)\n",
    "newdata = category_convert(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "605b6d8f6b3283dfd1d5ce4cd6b72111663c5647"
   },
   "outputs": [],
   "source": [
    "# historical_transactions & new merchants transaction\n",
    "# categorical data and other general data\n",
    "def aggregate_trans(data, prefix):  \n",
    "    agg_func = {\n",
    "        'card_id': ['size'], #num_trans\n",
    "        'authorized_flag': ['sum', 'mean','nunique'],\n",
    "        'category_1': ['sum', 'mean','nunique'],\n",
    "        'category_2': ['nunique'],\n",
    "        'category_3': ['nunique'],\n",
    "        'cat2_1.0': ['mean'],\n",
    "        'cat2_2.0': ['mean'],\n",
    "        'cat2_3.0': ['mean'],\n",
    "        'cat2_4.0': ['mean'],\n",
    "        'cat2_5.0': ['mean'],\n",
    "        'cat3_A': ['mean'],\n",
    "        'cat3_B': ['mean'],\n",
    "        'cat3_C': ['mean'],\n",
    "        'city_id': ['nunique'],\n",
    "        'state_id': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'installments': ['sum', 'mean','median', 'max', 'min', 'std', 'nunique'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        'merchant_category_id': ['nunique'],\n",
    "        'month_lag': ['mean', 'max', 'min', 'std', 'nunique'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std', 'nunique']\n",
    "    }    \n",
    "    agg_trans = data.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    \n",
    "    return agg_trans\n",
    "\n",
    "hist_sum = aggregate_trans(histdata, 'hist_')\n",
    "new_sum = aggregate_trans(newdata, 'new_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "faf6b762cf99959a51066d1480b99c2045d525da"
   },
   "outputs": [],
   "source": [
    "# Devide time \n",
    "# Code from: Chau Ngoc Huynh - \"My first kernel (3.699)\"\n",
    "\n",
    "def devide_time(data):\n",
    "    data['purchase_date'] = pd.to_datetime(data['purchase_date'])\n",
    "    data['month_diff'] = ((datetime.datetime.today() - data['purchase_date']).dt.days)//30  \n",
    "    data['purchase_year'] = data['purchase_date'].dt.year\n",
    "    data['purchase_month'] = data['purchase_date'].dt.month\n",
    "    data['weekofyear'] = data['purchase_date'].dt.weekofyear\n",
    "    data['dayofweek'] = data['purchase_date'].dt.dayofweek\n",
    "    data['weekend'] = (data.purchase_date.dt.weekday >=5).astype(int)\n",
    "    data['hour'] = data['purchase_date'].dt.hour\n",
    "    return data\n",
    "\n",
    "hist_times = devide_time(histdata)\n",
    "new_times = devide_time(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0ebdc5290e2b327b96f8b799c1160d98004f640"
   },
   "outputs": [],
   "source": [
    "def aggregate_times(data, prefix):  \n",
    "#     data.loc[:, 'purchase_date'] = pd.DatetimeIndex(data['purchase_date']).astype(np.int64) * 1e-9\n",
    "\n",
    "    agg_func = {\n",
    "#         'purchase_date': [np.ptp, 'min', 'max','nunique'],  #np.ptp=pur_term\n",
    "        'month_diff': ['mean','max','min'],\n",
    "        'purchase_year': ['mean', 'max', 'min', 'std','nunique'],\n",
    "        'purchase_month': ['mean', 'max', 'min', 'std','nunique'],\n",
    "        'weekofyear': ['mean','max','min','nunique'],\n",
    "        'dayofweek': ['mean'],\n",
    "        'weekend': ['sum', 'mean'],\n",
    "        'hour': ['mean','max','min']\n",
    "    }    \n",
    "    agg_times = data.groupby(['card_id']).agg(agg_func)\n",
    "    agg_times.columns = [prefix + '_'.join(col).strip() \n",
    "                           for col in agg_times.columns.values]\n",
    "    agg_times.reset_index(inplace=True)\n",
    "    \n",
    "    return agg_times\n",
    "\n",
    "hist_times = aggregate_times(hist_times, 'hist_')\n",
    "new_times = aggregate_times(new_times, 'new_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b551fecf8646c514fb02a2ae00aeba413aba2fb"
   },
   "outputs": [],
   "source": [
    "# purchase date term\n",
    "histdata['pur_date'] = pd.DatetimeIndex(histdata['purchase_date']).date\n",
    "newdata['pur_date'] = pd.DatetimeIndex(newdata['purchase_date']).date\n",
    "\n",
    "histdata.loc[:,'pur_date'] = pd.DatetimeIndex(histdata['pur_date']).astype(np.int64) * 1e-9\n",
    "newdata.loc[:,'pur_date'] = pd.DatetimeIndex(newdata['pur_date']).astype(np.int64) * 1e-9\n",
    "\n",
    "agg_fn= {\n",
    "        'pur_date': [np.ptp,'max','min'], # np.ptp: Range of values (maximum - minimum) \n",
    "        }\n",
    "agg_hist = histdata.groupby(['card_id']).agg(agg_fn)\n",
    "agg_hist.columns = ['_'.join(col).strip() for col in agg_hist.columns.values]\n",
    "agg_hist.reset_index(inplace=True)\n",
    "\n",
    "agg_new = newdata.groupby(['card_id']).agg(agg_fn)\n",
    "agg_new.columns = ['_'.join(col).strip() for col in agg_new.columns.values]\n",
    "agg_new.reset_index(inplace=True)\n",
    "\n",
    "agg_hist.columns = ['hist_' + c if c != 'card_id' else c for c in agg_hist.columns]\n",
    "agg_new.columns = ['new_' + c if c != 'card_id' else c for c in agg_new.columns]\n",
    "\n",
    "# scale agg_hist, agg_new\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "\n",
    "agg_hist['hist_pur_date_ptp']=sk.preprocessing.scale(agg_hist['hist_pur_date_ptp'])\n",
    "agg_new['new_pur_date_ptp']=sk.preprocessing.scale(agg_new['new_pur_date_ptp'])\n",
    "agg_hist['hist_pur_date_max']=sk.preprocessing.scale(agg_hist['hist_pur_date_max'])\n",
    "agg_new['new_pur_date_max']=sk.preprocessing.scale(agg_new['new_pur_date_max'])\n",
    "agg_hist['hist_pur_date_min']=sk.preprocessing.scale(agg_hist['hist_pur_date_min'])\n",
    "agg_new['new_pur_date_min']=sk.preprocessing.scale(agg_new['new_pur_date_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6fb67afab3f96768404e788c10f4c909dd0be24"
   },
   "outputs": [],
   "source": [
    "# merge \n",
    "hist = hist_times.merge(hist_sum,on='card_id',how='left')\n",
    "hist = hist.merge(agg_hist, on='card_id',how='left')\n",
    "del hist_sum\n",
    "del hist_times\n",
    "del agg_hist\n",
    "\n",
    "new = new_times.merge(new_sum, on='card_id',how='left')\n",
    "new = new.merge(agg_new, on='card_id',how='left')\n",
    "del new_sum\n",
    "del new_times\n",
    "del agg_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a422042ae3ee1f1b9654ea6a8f56bd58eba11273"
   },
   "outputs": [],
   "source": [
    "# # avg_term = as.integer(mean(abs(diff(order(purchase_date)))))\n",
    "# def avg_term_f(x):\n",
    "#     s = x.sort_values()\n",
    "#     y = abs(np.diff(s)).mean().tolist()\n",
    "#     return y\n",
    "\n",
    "# hist['hist_avg_term'] = histdata.groupby('card_id')['purchase_date'].apply(avg_term_f)\n",
    "# new['new_avg_term'] = newdata.groupby('card_id')['purchase_date'].apply(avg_term_f)\n",
    "# still working on...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48679769e8643f225f4c3a3ac4843cc164cd4b9d"
   },
   "outputs": [],
   "source": [
    "train = train.merge(hist, on='card_id',how='left')\n",
    "train = train.merge(new, on='card_id',how='left')\n",
    "\n",
    "test = test.merge(hist, on='card_id',how='left')\n",
    "test = test.merge(new, on='card_id',how='left')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "479f958a95147117b9b64c7ec49b612dc485a3f7"
   },
   "outputs": [],
   "source": [
    "# save featured data\n",
    "train.to_csv(\"train_featured.csv\", index=False)\n",
    "test.to_csv(\"test_featured.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "baf90f504b1529bd3921f62e4a32c3761514068d"
   },
   "outputs": [],
   "source": [
    "# drop card_id before running model\n",
    "train = train.drop('card_id', axis=1) #,'hist_avg_term','new_avg_term'\n",
    "test = test.drop('card_id', axis=1) #,'hist_avg_term','new_avg_term'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed159ef8dc8f78c0786b492123a27a789dcd0b48"
   },
   "source": [
    "## LightGBM Model\n",
    "Approach:\n",
    "\n",
    "1st round training: find out important features -> delete correlated features\n",
    "\n",
    "2nd round training: final prediction only with selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "75294563c582639d56ade9e901e556ed39fe7b66"
   },
   "source": [
    "### 1st round: run the model for extracting important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94d97903dec5be0b0f8cd572bfe44198bc1f778a"
   },
   "outputs": [],
   "source": [
    "# set default parameters for 1st round training\n",
    "params = {'boosting': 'gbdt',\n",
    "          'objective':'regression',\n",
    "          'metric': 'rmse',\n",
    "          'learning_rate': 0.01, # 0.003! #0.005 #0.006 \n",
    "          'num_leaves': 110, #110 #100 #150 large, but over-fitting\n",
    "          'max_bin': 66,  #60 #50 # large,but slower,over-fitting\n",
    "          'max_depth': 10, # deal with over-fitting\n",
    "          'min_data_in_leaf': 30, # deal with over-fitting\n",
    "          'min_child_samples': 20,\n",
    "          'feature_fraction': 0.5,#0.5 #0.6 #0.8\n",
    "          'bagging_fraction': 0.8,\n",
    "          'bagging_freq': 40,#5  \n",
    "          'bagging_seed': 11,\n",
    "          'lambda_l1': 2,#1.3! #5 #1.2 #1\n",
    "          'lambda_l2': 0.1 #0.1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91b0c81344c8c52f0c5111236719ae1e4fd793d8"
   },
   "outputs": [],
   "source": [
    "# Reference: code from Ashish Patel(阿希什)Repeated KFOLD Approach: RMSE[3.70]\n",
    "# Kfold cross-validation\n",
    "# folds = KFold(n_splits=5, shuffle=True, random_state=11)\n",
    "\n",
    "nfolds = 5\n",
    "nrepeats = 2 \n",
    "folds = RepeatedKFold(n_splits=nfolds, n_repeats=nrepeats, random_state=11)\n",
    "fold_pred = np.zeros(len(train))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "lgb_preds = np.zeros(len(test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values,target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx]) #categorical_feature=categorical_feats\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx]) #categorical_feature=categorical_feats\n",
    "\n",
    "    iteration = 2000\n",
    "    lgb_m = lgb.train(params, trn_data, iteration, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
    "    fold_pred[val_idx] = lgb_m.predict(train.iloc[val_idx], num_iteration=lgb_m.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = train.columns\n",
    "    fold_importance_df[\"importance\"] = lgb_m.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    lgb_preds += lgb_m.predict(test, num_iteration=lgb_m.best_iteration) / (nfolds*nrepeats)\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(np.sqrt(mean_squared_error(fold_pred, target))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ffe7c71f66445c8fa5b8077326c2f880dfcf453a"
   },
   "outputs": [],
   "source": [
    "# ranking all feature by avg importance score from Kfold, select top100\n",
    "all_features = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)\n",
    "all_features.reset_index(inplace=True)\n",
    "important_features = list(all_features[0:100]['feature'])\n",
    "all_features[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2091fa0e9b91d0a47e739238e23d6d961b93a383"
   },
   "outputs": [],
   "source": [
    "# Check feature correlation \n",
    "# important_features = list(final_importance['feature'][0:60])\n",
    "df = train[important_features]\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "high_cor = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print(len(high_cor))\n",
    "print(high_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3bdb72836d81b7c041345ad73b1652a985de19ac"
   },
   "outputs": [],
   "source": [
    "# final selected features: drop highly correlated features from important features.\n",
    "features = [i for i in important_features if i not in high_cor]\n",
    "print(len(features))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d97fdef1e722c9375286bc63f4568cf1fad29ca0"
   },
   "source": [
    "## 2nd round: Train model with selected important_features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62637b85b24579d2873eed03221b83d22c32a95c"
   },
   "outputs": [],
   "source": [
    "# params for 2nd round training\n",
    "params = {'boosting': 'gbdt',\n",
    "          'objective':'regression',\n",
    "          'metric': 'rmse',\n",
    "          'learning_rate': 0.003, # 0.003! #0.005 #0.006 \n",
    "          'num_leaves': 110, #110 #100 #150 large, but over-fitting\n",
    "          'max_bin': 66,  #60 #50 # large,but slower,over-fitting\n",
    "          'max_depth': 10, # deal with over-fitting\n",
    "          'min_data_in_leaf': 30, # deal with over-fitting\n",
    "          'min_child_samples': 20,\n",
    "          'feature_fraction': 0.8,#0.5 #0.6 #0.8\n",
    "          'bagging_fraction': 0.8,\n",
    "          'bagging_freq': 40,#5  \n",
    "          'bagging_seed': 11,\n",
    "          'lambda_l1': 2,#1.3! #5 #1.2 #1\n",
    "          'lambda_l2': 0.1 #0.1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f84de8e071a0158a0e6f8df0c6fef2702358923f"
   },
   "outputs": [],
   "source": [
    "train = train[features]\n",
    "test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "641844a43131b8c77c931cc7198e353c9abd28cd"
   },
   "outputs": [],
   "source": [
    "# Use Kfold predict\n",
    "nfolds = 5\n",
    "nrepeats = 2 \n",
    "\n",
    "folds = RepeatedKFold(n_splits=nfolds, n_repeats=nrepeats, random_state=11)\n",
    "fold_pred = np.zeros(len(train))\n",
    "lgb_preds = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)): #target.values\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx]) #categorical_feature=categorical_feats\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx]) #categorical_feature=categorical_feats\n",
    "\n",
    "    iteration = 3000\n",
    "    lgb_model = lgb.train(params, trn_data, iteration, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
    "    fold_pred[val_idx] = lgb_model.predict(train.iloc[val_idx], num_iteration=lgb_model.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = train.columns\n",
    "    fold_importance_df[\"importance\"] = lgb_model.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    lgb_preds += lgb_model.predict(test, num_iteration=lgb_model.best_iteration) / (nfolds*nrepeats)\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(np.sqrt(mean_squared_error(fold_pred, target))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96af73cd04fa5f4c72f8235a42d7d10332c14e99"
   },
   "outputs": [],
   "source": [
    "# training data label \n",
    "target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c84a5d94354e803e8dbb60373f0e152fe7de4ab"
   },
   "outputs": [],
   "source": [
    "# predicted values\n",
    "pd.DataFrame(lgb_preds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e97ae28515627e0002e528542a2e2607a97b691"
   },
   "outputs": [],
   "source": [
    "# predicted value distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_bins = 100\n",
    "n, bins, patches = plt.hist(lgb_preds, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40097b8507866ea171ab4a2dfad0a6b42341df0e"
   },
   "outputs": [],
   "source": [
    "# Add target value to submition file\n",
    "lgb_submition[\"target\"] = lgb_preds\n",
    "lgb_submition.to_csv(\"lgb_submition.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a83961ac3fb3b8fa4c332676e5b498a0c0715d48"
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "final_importance = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)\n",
    "final_importance.reset_index(inplace=True)\n",
    "final_importance[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ecdb29aecd2061d4d2468eba554a8270caa1cf7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",y=\"feature\",data=final_importance)\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
